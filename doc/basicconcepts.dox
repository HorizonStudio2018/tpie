/**
\page basicconcepts Overview and basic concepts

The data sets
involved in some modern applications are too large to fit in the main
memory of even the most powerful computers and must therefore reside
on disk.  Thus communication between internal and external memory, and
not actual computation time, often becomes the bottleneck in the
computation. This is due to the huge difference in access time of fast
internal memory and slower external memory such as disks. While
typical access time of main memory is measured in nanoseconds, a
typical access time of a disk is on the order of
milliseconds [<a href="#cockcroftsun">1</a>]. So roughly speaking there is a
factor of a million difference in the access time of internal and
external memory. A good example of an applications involving massive
amounts of geometric data is NASA's Earth Observation System
(EOS) [<a href="#cromp">2</a>, <a href="#koblernasa">3</a>], which is expected to manipulate
petabytes (thousands of terabytes, or millions of gigabytes) of data.

The goal of theoretical work in the area of <em>external memory (EM)
  algorithms</em> (also called <em>I/O algorithms</em> or <em>out-of-core
  algorithms</em>) is to eliminate or minimize the I/O bottleneck through
better algorithm design. In order to cope with the high cost of
accessing data, efficient EM algorithms exploit locality in their
design.  They access a large \em block of \em B contiguous data
elements at a time and perform the necessary algorithmic steps on the
elements in the block while in the high-speed memory. The speedup can
be considerable.  A second effective strategy for EM algorithms is the
use of multiple parallel disks; whenever an input/output operation is
performed, \em D blocks are transferred in parallel between memory and
each of the \em D disks (one block per disk).

The study of EM algorithm design was effectively started in the late
eighties by Aggarwal and Vitter [<a href="#aggarwalinput">4</a>] and an important
model for designing I/O algorithms called the Parallel Disk Model
(PDM) was later proposed by Vitter and Shriver [<a href="#vitterparmem1">5</a>].
The PDM proposed that a good EM algorithm should transfer data between
main memory and disk in a blocked manner, and should use all of the
available disks concurrently. An optimal EM algorithm under this model
minimizes the number of such blocked, parallel I/O operations it
performs.
 
Subsequently, I/O algorithms for the PDM (mostly with a single disk
and single processor) have been developed for many problem domains,
including computational
geometry [<a href="#aapv-fibld-01">6</a>, <a href="#goodrichexternal">7</a>, <a href="#argebuffer">8</a>, <a href="#argetheory">9</a>, <a href="#argegis">10</a>,
<a href="#aamvv-empgbtag97">11</a>, <a href="#argeinterval">12</a>, <a href="#kanellakisindexing">13</a>, <a href="#ramaswamypath">14</a>,
<a href="#subramanianp-range">15</a>, <a href="#vengroffefficient">16</a>, <a href="#agarwalefficient">17</a>, <a href="#zhufurther">18</a>,
<a href="#agarwalpoint">19</a>, <a href="#argescalable">20</a>, <a href="#argetheory">9</a>, <a href="#callahantopology">21</a>, <a href="#franciosaorders">22</a>,
<a href="#grossicross-tree">23</a>, <a href="#argetpie">24</a>], 
graph algorithms [<a href="#chiangexternal">25</a>, <a href="#argebuffer">8</a>, <a href="#kumarimproved">26</a>,
<a href="#abellofunctional">27</a>, <a href="#crauserrandomized">28</a>, <a href="#argeobdd">29</a>, <a href="#feuersteinmemory">30</a>,
<a href="#nodineblocking">31</a>, <a href="#ullmaninput">32</a>], 
and string processing [<a href="#ferraginafully">33</a>, <a href="#ferraginafast">34</a>, <a href="#argestrings">35</a>,
<a href="#crauserconstruction">36</a>].

The use of parallel disks
 has also received some
theoretical
attention [<a href="#vitterparmem1">5</a>, <a href="#nodinedeterministic">37</a>, <a href="#nodinegreed">38</a>, <a href="#dehneefficient">39</a>, <a href="#dehnereducing">40</a>].
There are more complicated models than the PDM, designed to address
the I/O bottleneck in different ways. These include models that
address the communication bottleneck between multiple layers in memory
hierarchies, and models incorporating
parallel processors as well as parallel
disks [<a href="#cormenchallenge">41</a>, <a href="#dehneefficient">39</a>, <a href="#dehnereducing">40</a>].

Implementations of these theoretical results are scarce. TPIE, <em>a
  Transparent Parallel I/O Environment</em>, is intended to bridge the gap
between the theory and practice of parallel I/O systems. On one hand,
TPIE attempts to provide usable implementations of (sometimes complex)
theoretical algorithms, feeding back that experience to algorithm
designers. On the other hand, TPIE also accommodates the use of
heuristics from the practice of I/O algorithms in order to achieve
maximum performance. Other EM implementation work includes
benchmarking of certain geometric I/O algorithms by
Chiang [<a href="#chiangexperiments">42</a>], experiments with FFT and related
algorithms by Cormen et al. [<a href="#cormenffts">43</a>], implementation of the
buffer tree [<a href="#argebuffer">8</a>] by Hutchinson
et al. [<a href="#hutchinsonearly">44</a>], and the LEDA-SM system for
implementing data types by Crauser et al.[<a href="#mehlhornledasm">45</a>].
Surveys of previous work in EM algorithm design and implementation can
be found in [<a href="#argegisbook">46</a>, <a href="#argethesis">47</a>, <a href="#vitterdimacssurvey">48</a>]

The objectives of the TPIE project include the following:

- <em>Abstract away the details of how I/O is performed</em> so that
  programmers need only deal with a simple high level interface.
- <em>Provide a collection of I/O-optimal paradigms</em> for large
  scale computation that are efficient not only in theory, but also in
  practice.
- <em>Be flexible</em>, allowing programmers to specify the
  functional details of computation taking place within the supported
  paradigms.  This will allow a wide variety of algorithms to be
  implemented within the system.
- <em>Be portable</em> across a variety hardware platforms.
- <em>Be extensible</em>, so that new features can be easily added
  later.

\section basicconcepts Basic concepts

TPIE is written in the C++ language, and this manual assumes that
the reader is familiar with C++.

Familiarity with the theoretical results on I/O-efficient algorithms
is not necessary in order to use TPIE. However, this manual may be easier to
follow with some general
background information such as how a theoretically optimal external
merge sort algorithm works. Some of the basic concepts
required for understanding the discussion of I/O issues and external
memory algorithms in this manual are outlined below.

Roughly speaking there is a factor of a million difference in the
access time of internal and external memory.  In order to cope with
the high cost of accessing externally-stored data, efficient EM
algorithms exploit locality in their design.  They access a large
\em block of \em B contiguous data elements at a time and perform
the necessary algorithmic steps on the elements in the block while it
is in the high-speed memory. The speedup can be considerable.

The performance of an EM algorithm on a given set of data is affected
directly by how much internal memory is available for its use. We use
\em M to denote the number of application data elements that fit into
the internal memory available to the algorithm, and \em m = \em M / \em B denotes
the number of blocks that fit into the available internal memory. Such
a block is more precisely called a <em>logical block</em> because it
may be a different size (usually larger) than either the physical
block size or the system block size. We will reserve the term
<em>physical block</em> size to mean the block size used by a disk
controller to communicate with a physical disk, and the <em>system
block</em> size will be the size of block used within the operating
system for I/O operations on disk devices. In EM algorithms we will
assume that the logical block size is a multiple of the system block
size.

TPIE is implemented as a set of templated classes and functions in
C++, and employs an object-oriented abstraction to EM computation.
TPIE provides C++ templates of various optimal EM computation
\em patterns or \em paradigms.  Examples of such paradigms are
the EM algorithms for merge sorting, distribution sweeping, time
forward processing, etc. In a TPIE
program, the application programmer provides application-specific
details of the specific computation paradigm used, such as C++
object definitions of the application data records, and code for
application-specific sub-computations at critical points in the
computation pattern, but TPIE provides the application-independent
parts of the pattern.

The definition of an application data element (or record) is provided
by the user as a class definition.  Such a class definition is
typically used as a template parameter in a TPIE code fragment (e.g. a
templated function).


\section stream Streams

In TPIE, a \em stream is an ordered collection of
objects of a particular type, stored in external memory, and accessed
in sequential order. Streams can be thought of as fundamental TPIE
objects which map volatile, typed application data elements in
internal memory to persistent, untyped data elements in external
memory, and vice-versa.  Streams are read and written like files in
Unix and support a number of primitive file-like operations such as
\c read(), \c write(), \c truncate(), etc.
TPIE also supports the concept of a \em substream, which
permits a contiguous subset of the elements in a stream to
accessed sequentially.

Various paradigms of external memory computation are supported.
TPIE reduces the
programming effort required to perform an external sort, merge, etc.,
by providing the high level flow of control within each paradigm, and
therefore structuring this part of the computation so that it will be
I/O efficient. The programmer is left with the task of providing what
amount to event handlers, specifying the application-specific
details of the computation. For instance in sorting, the programmer
defines a stream of input data, a comparison operator (the event handler
for the task of comparing two application data elements), and an
output stream for the results.

Creating a stream of objects in TPIE is very much like creating any
other object in C++. The only difference is that the data placed in
the stream is stored in external memory (on disk).

\section introreferences References

<a href="#back">^</a> <span id="cockcroftsun">1: A. Cockcroft, Sun Performance and Tuning. SPARC & Solaris</span>

<a href="#back">^</a> <span id="cromp">2: R. F. Cromp, An Intellegent Information Fusion System for Handling the Archiving and Querying of Terabyte-sized Spatial Databases</span>

<a href="#back">^</a> <span id="koblernasa">3: B. Kobler and J. Berbert, NASA Earth Observing Systems Data Information System (EOSDIS)</span>

<a href="#back">^</a> <span id="aggarwalinput">4: A. Aggarwal and J.~S. Vitter, The Input/Output Complexity of Sorting and Related Problems</span>

<a href="#back">^</a> <span id="vitterparmem1">5: J. S. Vitter and E. A. M. Shriver, Algorithms for Parallel Memory, I: Two-level Memories</span>

<a href="#back">^</a> <span id="aapv-fibld-01">6: Pankaj K. Agarwal and Lars Arge and Octavian Procopiuc and Jeffrey S. Vitter, A Framework for Index Bulk Loading and Dynamization</span>

<a href="#back">^</a> <span id="goodrichexternal">7: M. T. Goodrich and J.-J. Tsay and D. E. Vengroff and J. S. Vitter, External-Memory Computational Geometry</span>

<a href="#back">^</a> <span id="argebuffer">8: L. Arge, The Buffer Tree: A New Technique for Optimal I/O-Algorithms</span>

<a href="#back">^</a> <span id="argetheory">9: L. Arge and O. Procopiuc and S. Ramaswamy and T. Suel and J. S. Vitter, Theory and Practice of I/O-Efficient Algorithms for Multidimensional Batched Searching Problems</span>

<a href="#back">^</a> <span id="argegis">10: L. Arge and D. E. Vengroff and J. S. Vitter, External-Memory Algorithms for Processing Line Segments in Geographic Information Systems</span>

<a href="#back">^</a> <span id="aamvv-empgbtag97">11: P. K. Agarwal and L. Arge and T. M. Murali and K. Varadarajan and J. S. Vitter, I/O-Efficient Algorithms for Contour Line Extraction and Planar Graph Blocking</span>

<a href="#back">^</a> <span id="argeinterval">12: L. Arge and J. S. Vitter, Optimal Dynamic Interval Management in External Memory</span>

<a href="#back">^</a> <span id="kanellakisindexing">13: P. C. Kanellakis and S. Ramaswamy and D. E. Vengroff and J. S. Vitter, Indexing for Data Models with Constraints and Classes</span>

<a href="#back">^</a> <span id="ramaswamypath">14: S. Ramaswamy and S. Subramanian, Path Caching: A Technique for Optimal External Searching</span>

<a href="#back">^</a> <span id="subramanianp-range">15: S. Subramanian and S. Ramaswamy, The P-range Tree: A New Data Structure for Range Searching in Secondary Memory</span>

<a href="#back">^</a> <span id="vengroffefficient">16: D. E. Vengroff and J. S. Vitter, Efficient 3-D Range Searching in External Memory</span>

<a href="#back">^</a> <span id="agarwalefficient">17: P. K. Agarwal and L. Arge and J. Erickson and P. Franciosa and J. Vitter, Efficient Searching with Linear Constraints</span>

<a href="#back">^</a> <span id="zhufurther">18: B. Zhu, Further Computational Geometry in Secondary Memory</span>

<a href="#back">^</a> <span id="agarwalpoint">19: P. K. Agarwal and L. Arge and G. S. Brodal and J. S. Vitter, I/O-Efficient Dynamic Point Location in Monotone Planar Subdivisions</span>

<a href="#back">^</a> <span id="argescalable">20: L. Arge and O. Procopiuc and S. Ramaswamy and T. Suel and J. S. Vitter, Scalable Sweeping-Based Spatial Join</span>

<a href="#back">^</a> <span id="callahantopology">21: P. Callahan and M. T. Goodrich and K. Ramaiyer, Topology B-trees and Their Applications</span>

<a href="#back">^</a> <span id="franciosaorders">22: P. G. Franciosa and M. Talamo, Orders, $k$-sets and fast halfplane search on paged memory</span>

<a href="#back">^</a> <span id="grossicross-tree">23: R. Grossi and G. F. Italiano, Efficient cross-tree for external memory</span>

<a href="#back">^</a> <span id="argetpie">24: L. Arge and O. Procopiuc and J. S. Vitter, Implementing I/O-Efficient Data Structures Using TPIE</span>

<a href="#back">^</a> <span id="chiangexternal">25: Y.-J. Chiang and M. T. Goodrich and E. F. Grove and R. Tamassia and D. E. Vengroff and J. S. Vitter, External-Memory Graph Algorithms</span>

<a href="#back">^</a> <span id="kumarimproved">26: V. Kumar and E. Schwabe, Improved Algorithms and Data Structures for Solving Graph Problems in External Memory</span>

<a href="#back">^</a> <span id="abellofunctional">27: J. Abello and A. L. Buchsbaum and J. R. Westbrook., A functional approach to external graph algorithms</span>

<a href="#back">^</a> <span id="crauserrandomized">28: A. Crauser and P. Ferragina and K. Mehlhorn and U. Meyer and E. Ramos, Randomized external-memory algorithms for geometric problems</span>

<a href="#back">^</a> <span id="argeobdd">29: L. Arge, The I/O-Complexity of Ordered Binary-Decision Diagram Manipulation</span>

<a href="#back">^</a> <span id="feuersteinmemory">30: E. Feuerstein and A. Marchetti-Spaccamela, Memory paging for connectivity and path problems in graphs</span>

<a href="#back">^</a> <span id="nodineblocking">31: M. H. Nodine and M. T. Goodrich and J. S. Vitter, Blocking for External Graph Searching</span>

<a href="#back">^</a> <span id="ullmaninput">32: J. D. Ullman and M. Yannakakis, The input/output complexity of transitive closure</span>

<a href="#back">^</a> <span id="ferraginafully">33: P. Ferragina and R. Grossi, A fully-Dynamic Data Structure for External Substring Search</span>

<a href="#back">^</a> <span id="ferraginafast">34: P. Ferragina and R. Grossi, Fast String Searching in Secondary Storage: Theoretical Developments and Experimental Results</span>

<a href="#back">^</a> <span id="argestrings">35: L. Arge and P. Ferragina and R. Grossi and J. Vitter, On Sorting Strings in External Memory</span>

<a href="#back">^</a> <span id="crauserconstruction">36: A. Crauser and P. Ferragina, On the construction of suffix arrays in external memory</span>

<a href="#back">^</a> <span id="nodinedeterministic">37: M. H. Nodine and J. S. Vitter, Deterministic Distribution Sort in Shared and Distributed Memory Multiprocessors</span>

<a href="#back">^</a> <span id="nodinegreed">38: M. H. Nodine and J. S. Vitter, Greed Sort: An Optimal Sorting Algorithm for Multiple Disks</span>

<a href="#back">^</a> <span id="dehneefficient">39: F. Dehne and W. Dittrich and D. Hutchinson, Efficient External Memory Algorithms by SImulating Coarse-Grained Parallel Algorithms</span>

<a href="#back">^</a> <span id="dehnereducing">40: F. Dehne and D. Hutchinson and A. Maheshwari, Reducing I/O complexity by simulating coarse grained parallel algorithms</span>

<a href="#back">^</a> <span id="cormenchallenge">41: T. H. Cormen and M. T. Goodrich, Position Statement, ACM Workshop on Strategic Directions in Computing Research: Working Group on Storage I/O for Large-Scale Computing</span>

<a href="#back">^</a> <span id="chiangexperiments">42: Y.-J. Chiang, Experiments on the Practical I/O Efficiency of Geometric Algorithms: Distribution Sweep vs. Plane Sweep</span>

<a href="#back">^</a> <span id="cormenffts">43: Thomas H. Cormen and David M. Nicol, Performing Out-of-Core FFTs on Parallel Disk Systems</span>

<a href="#back">^</a> <span id="hutchinsonearly">44: D. Hutchinson and A. Maheshwari and J-R. Sack and R. Velicescu, Early Experiences in Implementing the Buffer Tree</span>

<a href="#back">^</a> <span id="mehlhornledasm">45: A. Crauser and K. Mehlhorn, LEDA-SM: Extending LEDA to Secondary Memory</span>

<a href="#back">^</a> <span id="argegisbook">46: L. Arge, External-Memory Algorithms with Applications in Geographic Information Systems</span>

<a href="#back">^</a> <span id="argethesis">47: L. Arge, Efficient External-Memory Data Structures and Applications</span>

<a href="#back">^</a> <span id="vitterdimacssurvey">48: J. S. Vitter, External Memory Algorithms and Data Structures</span>

*/
